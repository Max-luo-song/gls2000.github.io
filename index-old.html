<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhenyu Zhang</title>
  
  <meta name="author" content="Zhenyu Zhang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Zhenyu Zhang</name>
              </p>
              <p>I am currently an associate professor at school of Intelligent Science and Technology, Nanjing University, Suzhou campus, 
                where I work on computer vision, computer graphics and machine learning.
              </p>
              <p>
                I got my Ph.D degree from Department of Computer Science and Engineering, Nanjing University of Science and Technology in 2020, 
                supervised by <a href="https://scholar.google.com/citations?user=6CIDtZQAAAAJ">Jian Yang</a>. 
                In 2019, I spent 10 wonderful months as a visiting student at <a href="http://mhug.disi.unitn.it/index.php/publications/#/">MHUG</a> group in Unviversity of Trento, Italy, supervised by <a href="http://disi.unitn.it/~sebe/">Nicu Sebe</a>. 
                During 2020-2023, I worked as a staff research scientist at Tencent Youtu Lab.
              </p>
              <p style="text-align:center">
                <a href="mailto:zhangjesse@foxmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4daxK2AAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/zhenyu-zhang-66317719a">Linkedin</a> &nbsp/&nbsp
                <a href="data/CV_Zhenyu_Zhang.pdf">CV</a> 
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/Zhenyu.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/Zhenyu_square.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in computer vision, photography and reconstrcution. 
                Much of my research is about inferring the 3D model (depth, normal, mesh, point cloud, etc) and intrinsic cues (light, albedo, specular, roughness, etc) from images, 
                and rendering high-realistic photo based on such information. My recent works are focused on 3D face modeling and neural rendering.
              </p>

              <heading>Hiring</heading>
              <p>
                <b style="color:red;">We are looking for self-motivated PhD candidates! Please feel free to contact me through the email (attach your CV). 
                During the PhD career, you can have:</b>

                <ul>
                  <li><b style="text-align:left">joint supervision with well-known research institute (e.g., Tencent Youtu Lab, Alibaba DAMO Academy, Huawei, etc.)</b> </p></li>
                  <li><b style="text-align:left">a suitable training plan according to your preference</b> </p></li>
                  <li><b style="text-align:left">hand-in-hand help to publish your earlier papers</b> </p></li>
                  <li><b style="text-align:left">relatively flexible and free research space</b> </p></li>
                  <li><b style="text-align:left">chances to visit the outstanding groups overseas</b> </p></li>
                </ul>

                <b>We would not push hard, but you should always be self-driven for your own target, i.e., making solid and impactful contributions to the CV/AI community.</b>
              </p>
              <heading>Recent News</heading>
              
              <ul>
                <li><p style="text-align:left">03/2023 – 1 paper <a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf">NPF</a> on 3D face modeling are accepted by <b>CVPR'23</b> </b> </p></li>
                <li><p style="text-align:left">09/2022 – 1 paper <a href="=https://ojs.aaai.org/index.php/AAAI/article/download/25415/25187">DesNet</a> on depth completion are accepted by <b>AAAI'23</b> </b> </p></li>
                <li><p style="text-align:left">07/2022 – 2 papers (<a href="=https://arxiv.org/pdf/2107.13802">RigNet</a> and <a href="=https://arxiv.org/pdf/2203.09855">M^3PT</a> ) on depth completion are accepted by <b>ECCV'22</b> </b> </p></li>
                <li><p style="text-align:left">03/2022 – 2 papers on face reconstruction and neural rendering are accepted by <b>CVPR'22</b>, with the acceptance rate to be <b>25.3%</b> </p></li>
                <li><p style="text-align:left">01/2022 – I obtain <b>Outstanding Doctoral Thesis Honorable Mention of CCF</b> (China Computer Federation) 2021. See this <a href="https://www.ccf.org.cn/Awards/Awards/2022-01-05/752703.shtml">page</a> (in Chinese) </p></li>
                <li><p style="text-align:left">07/2021 – 1 paper on nighttime depth estimation accepted by <b>ICCV'21</b>, with the acceptance rate to be <b>25.9%</b> </p></li>
                <li><p style="text-align:left">07/2021 – Our <a href="https://arxiv.org/pdf/2003.11228.pdf"; style="color: #EE7F2D;"> <b>ASFD</b></a> on face detection is accepted by <b>ACM MM'21</b> </p></li>
                <li><p style="text-align:left">03/2021 – 1 paper on 3D face modelling accepted by <b>CVPR'21 (Oral!)</b>, with the acceptance rate to be <b>23.7%</b> </p></li>
                <li><p style="text-align:left">02/2020 – 2 papers accepted by <b>CVPR'20</b>, with the acceptance rate to be <b>22.1%</b> </p></li>
                <li><p style="text-align:left">11/2019 – 1 paper accepted by <b>AAAI'20</b>, with the acceptance rate to be <b>20.6%</b> </p></li>
                <li><p style="text-align:left">10/2019 – Our ECCV'18 extended paper is accepted by <b>TPAMI!</b> </p></li>
                <li><p style="text-align:left">04/2019 – Our paper on online depth adaptation is released at <a href="https://arxiv.org/pdf/1904.08462.pdf">Arxiv</a> </p></li>
                <li><p style="text-align:left">03/2019 – 1 paper accepted by <b>CVPR'19</b> </p></li>
                <li><p style="text-align:left">12/2018 – I join <a href="http://mhug.disi.unitn.it/index.php/publications/#/">MHUG</a> as a visiting student. Wonderful people and city! </p></li>
                <li><p style="text-align:left">06/2018 – 1 paper accepted by <b>ECCV'18</b> </p></li>
                </ul>
            </td>
          </tr>
        </tbody></table>
        

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          &nbsp &nbsp &nbsp &nbsp <heading>Publications</heading> (* equal contribution)

          <td style="padding:20px;width:25%;vertical-align:middle"> 
            
            <img src="./images/NPF.png" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf">
              <papertitle><b>Learning Neural Proto-face Field for Disentangled 3D Face Modeling In the Wild</b></papertitle>
              </a>
              <br>
              <b>Zhenyu Zhang</b>, Renwang Chen, Weijian Cao, Ying Tai, Chengjie Wang.              
              <br>
              <em><b>CVPR</b></em> 2023
              <br>
              <a href="http://openaccess.thecvf.com/content/CVPR2023/papers/Zhang_Learning_Neural_Proto-Face_Field_for_Disentangled_3D_Face_Modeling_in_CVPR_2023_paper.pdf">Paper</a> /
              <a href="https://www.youtube.com/watch?v=_Du5z4qKT0w">Video</a> 
              <p><font color="red">State-of-the-art 3D portrait modeling, robust to large pose, challenging light or makeup. Also state-of-the-art novel view synthetis performance.</font></p>
            </td><tr>

          <tr onmouseout="c5_stop()" onmouseover="c5_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='c5_image'>
                  <img src='images/PhyDIR1.PNG' width="200"></div>
                <img src='images/PhyDIR2.PNG' width="200">
              </div>
              <script type="text/javascript">
                function c5_start() {
                  document.getElementById('c5_image').style.opacity = "1";
                }

                function c5_stop() {
                  document.getElementById('c5_image').style.opacity = "0";
                }
                c5_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Physically-Guided_Disentangled_Implicit_Rendering_for_3D_Face_Modeling_CVPR_2022_paper.pdf">
                <papertitle>Physically-Guided Disentangled Implicit Rendering for 3D Face Modeling</papertitle>
              </a>
              <br>
                  <b>Zhenyu Zhang</b>, Yanhao Ge, Ying Tai, Weijian Cao, Renwang Chen, Kunlin Liu, Hao Tang, Xiaoming Huang, Chengjie Wang, Dongjin Huang, Zhifeng Xie.              
                  <br>
                  <em><b>CVPR</b></em> 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Physically-Guided_Disentangled_Implicit_Rendering_for_3D_Face_Modeling_CVPR_2022_paper.pdf">Paper</a> /
              <p><font color="red">3D face modeling is limited by classical graphics rendering, so that we let it benefit from a novel neural rendering approach.</font>
                
              </p>
            </td>
          </tr>
          
          <td style="padding:20px;width:25%;vertical-align:middle"> 
            
            <img src="./images/L2R.PNG" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Learning_To_Restore_3D_Face_From_In-the-Wild_Degraded_Images_CVPR_2022_paper.pdf">
              <papertitle><b>Learning to Restore 3D Face from In-the-Wild Degraded Images</b></papertitle>
              </a>
              <br>
              <b>Zhenyu Zhang</b>, Yanhao Ge, Ying Tai, Xiaoming Huang, Chengjie Wang, Hao Tang, Dongjin Huang, Zhifeng Xie.              
              <br>
              <em><b>CVPR</b></em> 2022
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Zhang_Learning_To_Restore_3D_Face_From_In-the-Wild_Degraded_Images_CVPR_2022_paper.pdf">Paper</a> /
              <p><font color="red">When restoring degraded faces, you need to restore the 3D geometry-aware effect.</font></p>
            </td><tr>

          <td style="padding:20px;width:25%;vertical-align:middle"> 
            
            <img src="./images/ICCV21.JPG" style="height: 120px; width: 200px; margin-top: 10px">
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2108.03830">
              <papertitle><b>Regularizing Nighttime Weirdness: Efficient Self-supervised Monocular Depth Estimation in the Dark</b></papertitle>
              </a>
              <br>
              Kun Wang*, <b>Zhenyu Zhang*</b>, Xiang Li, Jun Li, Baobei Xu, Jian Yang.              
              <br>
              <em><b>ICCV</b></em> 2021
              <br>
              <a href="https://arxiv.org/pdf/2108.03830.pdf">Paper</a> /
              <a href="https://github.com/w2kun/RNW">Code</a> 
              <p><font color="red">Nighttime is challenging, but we make it. Dataset and code are now released.</font></p>
            </td><tr>
          
          <tr onmouseout="c5_stop()" onmouseover="c5_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='c5_image'>
                  <img src='images/LAP2.png' width="200"></div>
                <img src='images/LAP.png' width="200">
              </div>
              <script type="text/javascript">
                function c5_start() {
                  document.getElementById('c5_image').style.opacity = "1";
                }

                function c5_stop() {
                  document.getElementById('c5_image').style.opacity = "0";
                }
                c5_stop()
              </script>
            </td>
            <td style="padding:10px;width:75%;vertical-align:middle">
              <a href="http://arxiv.org/abs/2106.07852">
                <papertitle>Learning to Aggregate and Personalize 3D Face from In-the-Wild Photo Collection</papertitle>
              </a>
              <br>
                  <b>Zhenyu Zhang</b>, Yanhao Ge, Renwang Chen, Ying Tai, Yan Yan, Jian Yang, Chengjie Wang, Jinlin Li and Feiyue Huang.              
                  <br>
                  <em><b>CVPR</b></em> 2021, &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Learning_To_Aggregate_and_Personalize_3D_Face_From_In-the-Wild_Photo_CVPR_2021_paper.pdf">Paper</a> /
              <a href="http://arxiv.org/abs/2106.07852">Arxiv</a> /
              <a href="https://github.com/TencentYoutuResearch/3DFaceReconstruction-LAP">Code</a> 
              <p><font color="red">Improving the non-parametric 3D face reconstruction by leveraging consistency of unconstrained photo collection.</font>
                
              </p>
            </td>
          </tr>
              
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./images/AAAI20_2.JPG" style="height: 120px; width: 200px; margin-top: 10px">
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/pdf/2003.11228.pdf">
                  <papertitle><b>Cross-modal attention network for temporal inconsistent audio-visual event localization</b></papertitle>
                  </a>
                  <br>
                  Hanyu Xuan, <b>Zhenyu Zhang</b>, Shuo Chen, Jian Yang, Yan Yan.              
                  <br>
                  <em><b>AAAI</b></em> 2020
                  <br>
                  <p>Audio-visual event localization on temporal inconsistent videos</p>
                </td><tr>

            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./images/PSD.JPG" style="height: 120px; width: 200px; margin-top: 10px">
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhou_Pattern-Structure_Diffusion_for_Multi-Task_Learning_CVPR_2020_paper.pdf">
                  <papertitle><b>Pattern-Structure Diffusion for Multi-Task Learning</b></papertitle>
                  </a>
                  <br>
                  Ling Zhou, Zhen Cui, Chunyan Xu, <b>Zhenyu Zhang</b>, Chaoqun Wang, Tong Zhang, Jian Yang.              
                  <br>
                  <em><b>CVPR</b></em> 2020
                  <br>
                  <p>A graph-based method to mine multi-task relationship.</p>
                </td><tr>
              
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./images/LPF2.JPG" style="height: 120px; width: 200px; margin-top: 10px">
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Online_Depth_Learning_Against_Forgetting_in_Monocular_Videos_CVPR_2020_paper.pdf">
                  <papertitle><b>Online Depth Learning against Forgetting in Monocular Videos</b></papertitle>
                  </a>
                  <br>
                  <b>Zhenyu Zhang</b>, Stephane Lathuiliere, Elisa Ricci, Nicu Sebe, Yan Yan, Jian Yang.              
                  <br>
                  <em><b>CVPR</b></em> 2020
                  <br>
                  <p><font color="red">Depth estimation method fails in new scenes, but we can continuously align it against domain shift.</font></p>
                </td><tr>
          
      <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./images/PAP.JPG" style="height: 120px; width: 200px; margin-top: 10px">
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Pattern-Affinitive_Propagation_Across_Depth_Surface_Normal_and_Semantic_Segmentation_CVPR_2019_paper.pdf">
                  <papertitle><b>Pattern-Affinitive Propagation across Depth, Surface Normal and Semantic Segmentation</b></papertitle>
                  </a>
                  <br>
                  <b>Zhenyu Zhang</b>, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, Jian Yang.              
                  <br>
                  <em><b>CVPR</b></em> 2019
                  <br>
                  <p><font color="red">Feel difficult to combine different tasks? Here we provide a pair-wise similarity based method to leverage multi-task correlation.</font></p>
                </td><tr>

      <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./images/TRL_PAMI.JPG" style="height: 120px; width: 200px; margin-top: 10px">
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8758995">
                  <papertitle><b>Joint Task-Recursive Learning for RGB-D Scene Understanding</b></papertitle>
                  </a>
                  <br>
                  <b>Zhenyu Zhang</b>, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang Li, Jian Yang.              
                  <br>
                  <em><b>TPAMI</b></em> 2019
                  <br>
                  <p>A recursive approach for joint-task learning in RGBD scenes.</p>
                </td><tr>
        
        <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./images/ECCV.JPG" style="height: 120px; width: 200px; margin-top: 10px">
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Zhenyu_Zhang_Joint_Task-Recursive_Learning_ECCV_2018_paper.pdf">
                  <papertitle><b>Joint Task-recursive Learning for Semantic Segmentation and Depth Estimation</b></papertitle>
                  </a>
                  <br>
                  <b>Zhenyu Zhang</b>, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang Li, Jian Yang.              
                  <br>
                  <em><b>ECCV</b></em> 2018
                  <br>
                  <p>A new framework for joint depth estimation & semantic segmentation.</p>
                </td><tr>
        
        <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./images/TIP.JPG" style="height: 120px; width: 200px; margin-top: 10px">
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/8331148">
                  <papertitle><b>Progressive Hard-Mining Network for Monocular Depth Estimation</b></papertitle>
                  </a>
                  <br>
                  <b>Zhenyu Zhang</b>, Chunyan Xu, Jian Yang, Junbin Gao, Zhen Cui.              
                  <br>
                  <em><b>TIP</b></em> 2018
                  <br>
                  <p>Improving monocular depth estimation by mining difficult regions.</p>
                </td><tr>
        
        <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="./images/pr.JPG" style="height: 120px; width: 200px; margin-top: 10px">
                <td style="padding:10px;width:75%;vertical-align:middle">
                  <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320318301869">
                  <papertitle><b>Deep hierarchical guidance and regularization learning for end-to-end depth estimation</b></papertitle>
                  </a>
                  <br>
                  <b>Zhenyu Zhang</b>, Chunyan Xu, Jian Yang, Ying Tai, Liang Chen.              
                  <br>
                  <em><b>Pattern Recognition</b></em> 2018
                  <br>
                  <p>A new framework for monocular depth estimation.</p>
                </td><tr>

          

        </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Service</heading>
              <p>
                I serve as a reviewer for
                <ul>
                  <li><p style="text-align:left">IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020, 2021, 2022, 2023</b> </p></li>
                  <li><p style="text-align:left">AAAI Conference on Artificial Intelligence (AAAI), 2021, 2022, 2023</b> </p></li>
                  <li><p style="text-align:left">European Conference on Computer Vision (ECCV), 2020, 2022</b> </p></li>
                  <li><p style="text-align:left">International Conference on Computer Vision (ICCV), 2023</b> </p></li>
                  <li><p style="text-align:left">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</b> </p></li>
                  <li><p style="text-align:left">IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</b> </p></li>
                </ul>
              </p>

            </td>
          </tr>
        </tbody></table>

        <p style="text-align:left"><strong><font size="4px"> &nbsp &nbsp &nbsp  Awards</font></strong></p>
		<ul>
		  <li><p style="text-align:left">2021, <a href="https://www.ccf.org.cn/Awards/Awards/2022-01-05/752703.shtml">Outstanding Doctoral Thesis</a> of CCF (China Computer Federation), honorable mention </p></li>  
			<li><p style="text-align:left">2017-2018, 2018-2019 National Graduate Scholarship (top 2%) </p></li>
		</ul><br>

	<p style="text-align:justify"><strong><font size="4px"> &nbsp &nbsp &nbsp Others</font></strong></p>
		<ul>
			
			<li>
        <p style="text-align:left"> I am a fan of drama. I used to be one of the organizers of modern drama troupe of NJUST.  </p></li>
      </ur>
      <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=230&t=n&d=jXVRFhdcBIptmA-uInIx4ipfYBGEvhLs18EfNz-o7Ro'></script>
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js" integrity="sha3848gBf6Y4YYq7Jx97PIqmTwLPin4hxIzQw5aDmUg/DDhul9fFpbbLcLh3nTIIDJKhx" crossorigin="anonymous"></script>
      <script src="script.js"></script><table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
        <tr>
          <td style="padding:0px">
            <br>
            <p style="text-align:center;font-size:small;">
            Last modified in Aug. 2021.
            For the style of my personal website, Please refer to the wonderful page from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
            <br>
            </p>
          </td>
        </tr>
      </td>
    </tr>
  </table>
</body>

</html>
